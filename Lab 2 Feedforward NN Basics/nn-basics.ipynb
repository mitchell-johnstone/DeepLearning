{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Basics of Feed-Forward Neural Networks\n",
    "\n",
    "In this lab, we will start to create a feed-forward neural network from scratch.\n",
    "We begin with the very basic computational unit, a perceptron,\n",
    "and then we will add more layers and increase the complexity of our network. Along the way, we will learn how a perceptron works, the benefits of adding more layers, the kind of transformations necessary for learning complex features and relationships from data, and why an object-oriented paradigm is useful for easier management of our neural network framework.\n",
    "\n",
    "We will implement everything from scratch in Python using helpful\n",
    "libraries such as NumPy and PyTorch (without using the autograd feature of PyTorch). The purpose of this lab and the following lab\n",
    "series is to learn how neural networks work starting from the most basic\n",
    "computational units and proceeding to deeper and more networks. This will help us better understand how other popular deep learning frameworks, such as PyTorch, work underneath. You should be able to easily understand and implement everything in this lab. If you are having trouble consult with your instructors as the next lab series will assume a perfect understanding of the basic feed-forward neural network material.\n",
    "\n",
    "The recommended Python version for this implementation is 3.7. Recommended reading: sections 4.1 and 4.2 of the book (https://www.d2l.ai/chapter_multilayer-perceptrons/index.html).\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A perceptron or artificial neuron is the most basic processing unit of feed-forward neural networks. A perceptron can be modeled as a single-layer neural network with an input vector $\\mathbf{x} \\in \\mathbb{R}^n$, a bias $b$, a vector of trainable weights $\\mathbf{w} \\in \\mathbb{R}^n$, and an output unit $y$. Given the input $\\mathbf{x}$, the output $y$ is computed by an activation function $f(\\cdot)$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i w_i\\right) + b \\right) = f(\\mathbf{w}^\\intercal \\mathbf{x} + b)\\,,\n",
    "\\end{equation}\n",
    "where $\\Theta = \\{\\mathbf{w}, b\\}$ represents the trainable parameter set. \n",
    "\n",
    "The figure below shows a schematic view of a single output perceptron. Each input value $x_i$ is multiplied by a weight factor $w_i$. The weighted sum added to the bias is then passed through an activation function to obtain the output, $y$.\n",
    "\n",
    "![MLP example](img/perceptron.png)\n",
    " \n",
    "The vector $\\mathbf{x}$ represents one sample of our data and each element $x_i$ represents a feature. Thus, $\\mathbf{x}$ is often referred to as a feature vector. These features can represent different measurements depending on the application. For example, if we are trying to predict if a patient is at high risk of cardiac disease then each element of $\\mathbf{x}$ might contain vital signs such as diastolic and systolic blood pressure, heart rate, blood sugar levels, etc. In another application where we are trying to predict if a tissue biopsy is cancerous or not using mid-infrared imaging then each element of $\\mathbf{x}$ can represent the amount of mid-infrared light absorbed at a particular wavelength. The output $y$ in the applications above could contain values of $0$ or $1$, indicating if the patient is at high risk of cardiac disease or if the tissue biopsy is cancerous or not.\n",
    " \n",
    "Now, let us begin implementing our first artificial neuron.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's assume that our feature vector contains measurements of body temperature pressure, pulse oximeter reading, and presence of cough or not. Then for a 'healthy' patient our input sample might look like $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\end{bmatrix}$. Let's say that we are trying to 'predict' the probability of a patient being positive with COVID-19 based on the above measurements.\n",
    "\n",
    "Each element of our input vector is associated with a unique weight. Let the vector of weights be $\\mathbf{w} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\end{bmatrix}$. Each artifical neuron is also associated with a unique bias. Let the bias be $b = 2.9$. Assuming a linear activation function write the code to produce and print the output $y$ given the above input vector $\\mathbf{x}$, weights $\\mathbf{w}$, and the bias $b$ using the above perceptron model. Do not use any NumPy or PyTorch functions. Use a Python variable for each element and use Python lists for vectors.\n",
    "\n",
    "For the activation function, use ReLU. This can be computed as ```x * (x > 0)``` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.108000000000004\n"
     ]
    }
   ],
   "source": [
    "x = [98.6, 95, 0]\n",
    "W = [0.03, 0.55, 0.88]\n",
    "b = 2.9\n",
    "\n",
    "relu = lambda x : x * (x>0)\n",
    "\n",
    "result = b\n",
    "for i in range(len(x)):\n",
    "    result += x[i] * W[i]\n",
    "    \n",
    "print(relu(result))\n",
    "\n",
    "#print(relu(b+sum(map(lambda x,w:x*w, x, W))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 1: How many parameters does our simple model contain? Be specific.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 4 parameters total? This is broken down into 3 weights (w) and 1 bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 2: Recall that we were hoping to 'predict' the probability of a patient being positive with COVID-19. Does the output make sense? If not, elaborate on how you could fix it.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: The output doesn't really tell me anything. The output isn't a percentage or anything, and I don't have a reference for the bounds of the inputs, so I can't guess about the bounds of the outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Multiple Outputs\n",
    "\n",
    "The perceptron model above has only one output. However, in most applications, we need multiple outputs. For example, in a classification problem, we would expect the model to output a vector $\\mathbf{y}$, where each $y_i$ represents the probability of a sample belonging to a particular class $i$. The figure below shows a schematic view of a multiple output feed-forward neural network. Each input value $x_i$ is multiplied by a weight factor $W_{ij}$, where $W_{ij}$ denotes a connection weight between the input node $x_i$ and the output node $y_j$. The weighted sum is added to the bias and then passed through an activation function to obtain the output, $y_j$.\n",
    "\n",
    "![Multi outout perceptron](img/multi-output-perceptron.png)\n",
    "\n",
    "Given an input $\\mathbf{x} \\in \\mathbb{R}^n$ this can be modeled as:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i W_{ij}\\right) + b_j\\right) = f(\\mathbf{w}_j^\\intercal \\mathbf{x} + b_j)\\,,\n",
    "\\end{equation}\n",
    "where the parameter set here is $\\Theta = \\{ \\mathbf{W} \\in \\mathbb{R}^{n \\times m}, \\mathbf{b} \\in \\mathbb{R}^m \\}$ and $\\mathbf{w}_j$ denotes the $j^{th}$ column of $\\mathbf{W}$. \n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "\n",
    "Let $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\\\ 1 \\end{bmatrix}$. Let the output vector $\\mathbf{y} \\in \\mathbb{R}^3$, i.e. consisting of $3$ outputs. Let the weights associated with each output node $y_i$ be $\\mathbf{w_1} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\\\0.73 \\end{bmatrix}$, $\\mathbf{w_2} = \\begin{bmatrix} 0.48 \\\\ 0.31 \\\\ 0.28 \\\\ -0.9 \\end{bmatrix}$, $\\mathbf{w_3} = \\begin{bmatrix} 0.77 \\\\ 0.54 \\\\ 0.32 \\\\ 0.44 \\end{bmatrix}$. Let the bias vector be $\\mathbf{b} = \\begin{bmatrix} 2.9 \\\\ 6.1 \\\\ 3.3 \\end{bmatrix}$. Note that a single bias is associated with each output node $y_i$.\n",
    "\n",
    "Given the above inputs write the code to print the output vector $\\mathbf{y}$.  Use a Python variable for each scalar and use Python lists for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58.838, 82.07799999999999, 130.96200000000002]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Your code here\n",
    "x = [98.6, 95, 0, 1]\n",
    "W = [[0.03, 0.55, 0.88, 0.73], # W1\n",
    "     [0.48, 0.31, 0.28, -0.9], # W2\n",
    "     [0.77, 0.54, 0.32, 0.44]] # W3\n",
    "b = [2.9, 6.2, 3.3]\n",
    "\n",
    "def y_i(x, w, b):\n",
    "    return b+sum(map(lambda x,w:x*w, x, w))\n",
    "\n",
    "y = [y_i(x, W[i], b[i]) for i in range(len(b))]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Now that you understand how to do basic computations with a simple perceptron model manually, we will proceed to implement the same model above using matrix-vector operations utilizing PyTorch functions. Organizing the computations in matrix-vector format notation makes it simpler to understand and implement neural network models. \n",
    "\n",
    "Write the code to create the same output vector $\\mathbf{y}$ as above by expressing the above computations as matrix-vector multiplications and summation with a bias vector using code vectorization in PyTorch. You should get the same output as above, up to floating-point errors. Again use a Python variable for each scalar, but use PyTorch arrays for vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 58.8380,  82.0780, 130.9620])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Your code here\n",
    "\n",
    "x_t = torch.Tensor(x)\n",
    "W_1t = torch.Tensor(W)\n",
    "b_1t = torch.Tensor(b)\n",
    "\n",
    "y_1t = W_1t@x_t+b_1t\n",
    "y_1t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### <font color=blue>Question 3: Explain what each of the dimensions of the matrix of weights $\\mathbf{W}$ and the vector of biases $\\mathbf{b}$ represent?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: W is a 3x4 matrix. b is a vector of length 3. Thus, each of the dimensions of length 3 describe the desired output dimension. Meanwhile, the dimension of 4 on W is matching the input size. Thus, W is casting the inputs from R^4 to R^3, and then b is shifting the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 4: What is the total number of parameters for this model?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 3x4+3 = 15 total parameters? If we're only including w and b as a parameters of the model, then we have a total of 15 parameters being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "## More Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "A single-layer perceptron network still represents a linear classifier, even if we were to use nonlinear activation functions. This limitation can be overcome by multi-layer neural networks in combination with nonlinear activation functions, which introduce one or more 'hidden' layers between the input and output layers. Multi-layer neural networks are composed of several simple artificial neurons such that the output of one acts as the input of another. A multi-layer neural network can be represented by a composition function. For a two-layer network with only one output, the composition function can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(2)}\\left(\\sum_{k=1}^{h}W_{kj}^{(2)}*f^{(1)}\\left(\\left(\\sum_{i=1}^{n}W_{ik}^{(1)}*x_i \\right)+b_k^{(1)}\\right)+b_j^{(2)}\\right)\n",
    "\\end{equation}\n",
    "where $h$ is the number of units in the hidden layer and the set of unknown parameters is $\\Theta = \\{\\mathbf{W}^{(1)} \\in R^{n \\times h}, \\mathbf{W}^{(2)} \\in R^{h \\times 1}\\}$. In general, for $L - 1$ hidden layers the composition function, omitting the bias terms, can be written as\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(L)}\\left(\\sum_k W_{kj}^{L}*f^{L-1}\\left(\\sum_{l}W_{lk}^{L - 1}* f^{L - 2}\\left( \\cdots f^{1}\\left(\\sum_{i}W_{iz}^{1}*x_i \\right)\\right) \\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The figure below illustrates a feed-forward neural network composed of an input layer, a hidden layer, and an output layer. In this illustration, the multi-layer neural network has one input layer and one output unit. In most models, the number of hidden layers and output units is more than one.\n",
    "\n",
    "![Feed forward perceptron](img/feed-forward.png)\n",
    "\n",
    "We will now see how to add an additional layer to our model and then how to generalize to any number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add another layer we need another set of weights and biases. \n",
    "\n",
    "W_2 = [[-0.3, 0.66, 0.98],\n",
    "       [0.58, -0.4, 0.38],\n",
    "       [0.87, 0.69, -0.4]]\n",
    "\n",
    "b_2 = [3.9, 8.2, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "origin_pos": 28
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([168.7628,  59.2604,  56.2381])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Write the code to print the output of a 2-layer feed-forward network using the previously computed output,\n",
    "# y, as input to the second layer \n",
    "\n",
    "\n",
    "W_2t = torch.Tensor(W_2)\n",
    "b_2t = torch.Tensor(b_2)\n",
    "\n",
    "\n",
    "y_2 = W_2t@y_1t+b_2t\n",
    "y_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "### <font color=blue>Question 5: Explain the dimensions of the weight matrix for the second layer with respect to the dimensions of the previous layer and the number of artificial neurons in the second layer. or Why are the dimensions of the weight matrix of the second layer 3x3?</font>\n",
    "\n",
    "Ans: \n",
    "The previous result of y had a dimension of 3. This indicates that we have a hidden layer with 3 features. Then, we wanted our output to also have 3 features. Thus, the weight matrix had to transition from R^3 to R^3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers to Objects\n",
    "\n",
    "Now, we have a feed-forward model (with an input layer, one hidden layer, and one output layer with 3 outputs) capable of processing a batch of data. It would be cumbersome and redundant if we had to keep writing the same code for hundreds of layers. So, to make our code more modular, easier to manage, and less redundant we will represent layers using an object-oriented programming paradigm. Let's define classes for representing our layers.\n",
    "\n",
    "All layer objects should have an `output` instance attribute.  Use good object-oriented practices to avoid code duplication.  To initialize an instance attribute in Python, write `self.attribute_name = attribute_value` in the initializer (`__init__` method).  Don't mention the variable at the top of the class as we would usually do in Java -- this is how you define static attributes in Python.\n",
    "\n",
    "Rather than each layer taking PyTorch arrays as inputs, it should take `Layer`s as inputs, with each layer having its own name. For example, if your network would take $\\mathbf{x}$, $\\mathbf{W}$, and $\\mathbf{b}$ as inputs, you should have attributes `self.x`, `self.W`, and `self.b`.  Then, when you need the values of these inputs, go back and read the output of the previous layer.  For example, if your layer needs the value of $\\mathbf{W}$, you could read `self.W.output` to get it.\n",
    "\n",
    "Two more Python OO hints: (1) `class MyClass1(MyClass2)` is not a constructor call. It is specifying the inheritance relationship. The Java equivalent is `class MyClass1 extends MyClass2`. So you don't want to add arguments on this line. An easy mistake to make.  (2) You must use `self.` every time you access an instance variable in Python. This is how the language was designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the following classes.\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        TODO: Initialize instance attributes here.\n",
    "        \n",
    "        :param output_shape (tuple): the shape of the output array.  When this isa single number, it gives the number of output neurons\n",
    "            When this is an array, it gives the dimensions of the array of output neurons.\n",
    "        \"\"\"\n",
    "        self.output = np.random.random_sample(output_shape)\n",
    "        \n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "\n",
    "    def set(self,value):\n",
    "        \"\"\"\n",
    "        TODO: set the `output` of this array to have value `value`.\n",
    "        Raise an error if the size of value is unexpected. An `assert()` is fine for this.\n",
    "        \"\"\"\n",
    "        assert value.shape == self.output.shape\n",
    "        self.output = value\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, x_layer, W_layer, b_layer):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        Raise an error if any of the argument's size do not match as you would expect.\n",
    "        \"\"\"\n",
    "        assert x_layer.output.shape[0] == W_layer.output.shape[1]\n",
    "        assert W_layer.output.shape[0] == b_layer.output.shape[0]\n",
    "        Layer.__init__(self, b_layer.output.shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: Set this layer's output based on the outputs of the layers that feed into it.\n",
    "        \"\"\"\n",
    "        self.output = W_layer.output@x_layer.output+b_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " output of hidden layer \n",
      " [ 58.838  82.078 130.962]\n"
     ]
    }
   ],
   "source": [
    "# This example uses your classes to test the output of the entire network.\n",
    "#\n",
    "# You may change this example to match your own implementation if you wish.\n",
    "\n",
    "x_layer = Input(4)\n",
    "x_layer.set(np.array(x))\n",
    "W_layer = Input((3, 4))\n",
    "W_layer.set(np.array(W))\n",
    "b_layer = Input(3)\n",
    "b_layer.set(np.array(b))\n",
    "linear_layer = Linear(x_layer, W_layer, b_layer)\n",
    "\n",
    "linear_layer.forward()\n",
    "print('\\n output of hidden layer \\n', linear_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this lab... except for the two **required** parting questions:\n",
    "\n",
    "### <font color=blue>Question 6: Summarize what you learned during this lab.\n",
    "\n",
    "Ans: Perceptrons are artificial neurons. They form the basics of a single layer, and are defined generally so we can use them as a template for many other operations. Then, by feading the output of one perceptron as the input to the next perceptron, we can create multiple layers. This creates deep neural networks, since they're non-linear.\n",
    "\n",
    "Otherwise, I learned that OOPs is good for working with neural networks for simplicity in code and efficiency. It'll be interesting to see how we set this up going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 7: Describe what you liked about this lab *or* what could be improved. (Required.)\n",
    "\n",
    "Ans: I liked seeing the math. It explained it very nicely. I thought the ending exercise was a bit odd, with the setting of the values and whatnot, but otherwise I thought the lab had a good direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
